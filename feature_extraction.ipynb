{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
=======
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
>>>>>>> 3335707eac72030c71c4b97b15be68884e3c1270
    "from vgg16_hybrid_places_1365 import VGG16_Hubrid_1365\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from places_utils import preprocess_input\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 661,
=======
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width , img_height = 224, 224\n",
    "data_size = 3119 \n",
    "# number of times each image is changed through different data agumentation techniques\n",
    "# meaning from each images we get 4 images that are altred in some way\n",
    "data_agum_size = 4\n",
    "\n",
    "# data paths \n",
    "images_path = '/practical_stuff/all_images/'\n",
    "labels_path = 'practical_stuff/labels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pre-trained vgg16 model which is trained on images from Places365 database and imagenet data sets\n",
    "# loading this pre-trained vgg16 can take some time, approximately 5-10 minutes\n",
    "model = VGG16_Hubrid_1365(weights ='places', include_top=False, input_shape = (img_width, img_height, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
>>>>>>> 3335707eac72030c71c4b97b15be68884e3c1270
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>other</td>\n",
       "      <td>2378600.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>other</td>\n",
       "      <td>2378687.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>other</td>\n",
       "      <td>2379071.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>other</td>\n",
       "      <td>2379113.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>other</td>\n",
       "      <td>2379591.jpeg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class          image\n",
       "0  other   2378600.jpeg\n",
       "1  other   2378687.jpeg\n",
       "2  other   2379071.jpeg\n",
       "3  other   2379113.jpeg\n",
       "4  other   2379591.jpeg"
      ]
     },
<<<<<<< HEAD
     "execution_count": 661,
=======
     "execution_count": 498,
>>>>>>> 3335707eac72030c71c4b97b15be68884e3c1270
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "# read images information\n",
=======
    "# read labels\n",
>>>>>>> 3335707eac72030c71c4b97b15be68884e3c1270
    "labels = pd.read_csv(labels_path, sep = ',', header = None, names = ['class','image'])\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 633,
=======
   "execution_count": 499,
>>>>>>> 3335707eac72030c71c4b97b15be68884e3c1270
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3119, 2)\n"
     ]
    }
   ],
   "source": [
    "print (labels.shape)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save some computation time we can reduce sizes of images to (224, 224)\n",
    "img_width , img_height = 640, 640\n",
    "\n",
    "# number of times each image is changed through different data agumentation techniques\n",
    "# meaning from each images we get 4 images that are altred in some way\n",
    "data_agum_size = 4\n",
    "counter = 0\n",
    "# data paths \n",
    "images_path = '/practical_stuff/all_images/'\n",
    "labels_path = 'practical_stuff/labels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 640, 640, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 640, 640, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 640, 640, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 320, 320, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 320, 320, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 320, 320, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 160, 160, 128)     0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 160, 160, 256)     295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 160, 160, 256)     590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 160, 160, 256)     590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 80, 80, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 80, 80, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 80, 80, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 80, 80, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 40, 40, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 40, 40, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 40, 40, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 40, 40, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 20, 20, 512)       0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load a pre-trained vgg16 model which is trained on images from Places365 and imagenet data set\n",
    "# loading this pre-trained vgg16 can take some time\n",
    "model = VGG16_Hubrid_1365(weights ='places', include_top=False, input_shape = (img_width, img_height, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204800"
      ]
     },
     "execution_count": 663,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need the shape of the last layer which represents each image\n",
    "# this shape defines the shape of the feature\n",
    "# meaning each image will be represented by feaures that is feature_size long\n",
    "feature_size = np.product(model.layers[-1].get_output_at(0).get_shape().as_list()[1:])\n",
    "feature_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
=======
   "execution_count": 500,
>>>>>>> 3335707eac72030c71c4b97b15be68884e3c1270
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data augmentation \n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range = 40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.3,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 665,
=======
   "execution_count": 501,
>>>>>>> 3335707eac72030c71c4b97b15be68884e3c1270
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs data augmentation on images\n",
    "def data_agumentation(img, data_agum_size):\n",
<<<<<<< HEAD
    "    i = 0\n",
    "    img_agumented = np.zeros((data_agum_size, img_width, img_height,3))\n",
    "    for batch in datagen.flow(img):\n",
=======
    "    img_agumented = np.zeros((data_agum_size, img_width, img_height,3))\n",
    "    for batch in datagen.flow(img):\n",
    "        #print (\"shape of batches\",batch[0].shape)\n",
>>>>>>> 3335707eac72030c71c4b97b15be68884e3c1270
    "        img_agumented[i] = preprocess_input(batch[0].reshape((1,batch[0].shape[0],batch[0].shape[1],batch[0].shape[2])))\n",
    "        i += 1\n",
    "        if i >= data_agum_size:\n",
    "            break\n",
    "    return img_agumented"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the class lables for a given row image\n",
=======
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the class lables for a given row \n",
>>>>>>> 3335707eac72030c71c4b97b15be68884e3c1270
    "def get_target(row):\n",
    "    label = row['class']\n",
    "    if label == 'other':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from each row of images or from the augmented images\n",
    "def extract_features(row, features, targets, augmented = False):    \n",
    "    if augmented:\n",
    "        global counter\n",
=======
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from the \n",
    "def extract_features(row, features,targets, augmented = False):\n",
    "    global counter\n",
    "    \n",
    "    if augmented:\n",
>>>>>>> 3335707eac72030c71c4b97b15be68884e3c1270
    "        data_agum = np.zeros((img_width, img_height,3))\n",
    "    \n",
    "    image_path = os.path.join(os.getcwd() + images_path, row['image'].strip())\n",
    "    # load an image\n",
    "    img = image.load_img(image_path,target_size = (img_width, img_height))\n",
    "    \n",
    "    # convert the image into np array\n",
    "    array_image = image.img_to_array(img)\n",
    "    array_image = np.expand_dims (array_image, axis =0)\n",
<<<<<<< HEAD
    "    \n",
    "    if augmented:\n",
    "        # do data agumentation on this specific image. \n",
    "        # The resulting images are also pre-processed\n",
=======
    "    #print (array_image.shape)\n",
    "    \n",
    "    if augmented:\n",
    "        # do data agumentation on this specific image. the resulting images are also pre-processed\n",
>>>>>>> 3335707eac72030c71c4b97b15be68884e3c1270
    "        agumented = data_agumentation(array_image,data_agum_size)\n",
    "   \n",
    "    # process the image, basically standardization\n",
    "    array_image = preprocess_input(array_image)\n",
    "    \n",
    "    # extract the features by runnung the image array throught the pre-trained vgg16 model\n",
    "    img_features = model.predict(array_image)\n",
    "   \n",
<<<<<<< HEAD
    "    # get the class label for this image\n",
    "    class_label  = get_target(row)\n",
    "    features[counter] = img_features.reshape((feature_size))\n",
=======
    "   \n",
    "    # get the class label for this image\n",
    "    class_label  = get_target(row)\n",
    " \n",
    "    features[counter] = img_features.reshape((7*7*512))\n",
>>>>>>> 3335707eac72030c71c4b97b15be68884e3c1270
    "    targets [counter] = class_label\n",
    "    \n",
    "    counter +=1\n",
    "    \n",
    "    if augmented:\n",
    "        for img in agumented:\n",
    "            f_agumented = model.predict(img.reshape(1,img.shape[0], img.shape[1], img.shape[2] ))\n",
<<<<<<< HEAD
    "            features[counter] = f_agumented.reshape((feature_size))\n",
    "            targets[counter] = class_label\n",
    "            counter +=1"
=======
    "            features[counter] = f_agumented.reshape((7*7*512))\n",
    "            targets[counter] = class_label\n",
    "            counter +=1\n",
    "    print (counter)"
>>>>>>> 3335707eac72030c71c4b97b15be68884e3c1270
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving features and targets for further training\n",
    "# a classifier will be trained on these features and targets\n",
    "def save_features_targets(name, features, targets):\n",
    "    np.save('features_train', features_train)\n",
    "    np.save('targets_train', targets_train)   "
=======
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make sure results are reproducible, seed the random number generator\n",
    "# first shuffle the whoe data set and then \n",
    "# split the dataframe into two random samples (80% and 20%) for training and validation.\n",
    "np.random.seed (100)\n",
    "mask = np.random.rand(labels.shape[0]) < 0.8\n",
    "permutation = np.random.permutation(labels.shape[0])\n",
    "\n",
    "shuffled_dataset = labels.loc[permutation]\n",
    "train = shuffled_dataset[mask]\n",
    "valid = shuffled_dataset[~mask]"
>>>>>>> 3335707eac72030c71c4b97b15be68884e3c1270
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 670,
=======
   "execution_count": 504,
>>>>>>> 3335707eac72030c71c4b97b15be68884e3c1270
   "metadata": {},
   "outputs": [],
   "source": [
    "# place holders for features and the crosponding targets \n",
<<<<<<< HEAD
    "# each image has a feature that is feaature_size long. \n",
    "# Note: feature_size is the size of the last layer of the pre-trained vgg16 model.\n",
    "def place_holders(data_size, augmentation):\n",
    "    if augmentation:\n",
    "        features = np.zeros((data_size*data_agum_size + data_size, feature_size))\n",
    "        targets  = np.zeros((data_size*data_agum_size + data_size, 1))\n",
    "    \n",
    "    else:\n",
    "        features = np.zeros((data_size, feature_size))\n",
    "        targets  = np.zeros((data_size, 1))    \n",
    "    \n",
    "    return features, targets    "
=======
    "# each image has a feature that is 7*7*512 long. \n",
    "# Note: 7*7*512 is the size of the last layer of the pre-trained vgg16 model.\n",
    "# this pre-trained model doesn't have the FC layer\n",
    "# targets has shape of (3119, 1) as we have 3119 images\n",
    "data_size = train.shape[0]\n",
    "features_train = np.zeros ((data_size*data_agum_size + data_size, 7*7*512))\n",
    "targets_train = np.zeros ((data_size*data_agum_size + data_size, 1))"
>>>>>>> 3335707eac72030c71c4b97b15be68884e3c1270
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(augmentation):\n",
    "    \n",
    "    if augmentation:\n",
    "        # to make sure results are reproducible, seed the random number generator\n",
    "        # first shuffle the whole data set and then \n",
    "        # split the dataframe into two random samples (80% and 20%) for training and validation.\n",
    "        np.random.seed (100)\n",
    "        mask = np.random.rand(labels.shape[0]) < 0.8\n",
    "        permutation = np.random.permutation(labels.shape[0])\n",
    "\n",
    "        shuffled_dataset = labels.loc[permutation]\n",
    "        train = shuffled_dataset[mask]\n",
    "        valid = shuffled_dataset[~mask]\n",
    "    \n",
    "        # apply data augmentation only on the training part of the data set\n",
    "        # for each row of image call extract_features method\n",
    "        # this call might take some time, but it is a one time process\n",
    "        data_size = train.shape[0]\n",
    "        feature_train, target_train = place_holders(data_size, augmentation)\n",
    "        print ('feature_train shape', feature_train.shape)\n",
    "        train.apply(lambda x: extract_features(x, feature_train,target_train, True), axis =1)\n",
    "        save_features_targets('feature_train', feature_train)\n",
    "        save_feature_targets ('traget_train', target_train)\n",
    "        \n",
    "        # do not apply data augmentation on the validation part of the data set\n",
    "        data_size = valid.shape[0]\n",
    "        feature_valid, target_valid  = place_holders(data_size,augmentation)\n",
    "        valid.apply(lambda x: extract_features(x, feature_valid,target_valid), axis =1)\n",
    "        save_features_targets('feature_valid', feature_valid)\n",
    "        save_feature_targets ('traget_valid', target_valid)\n",
    "    \n",
    "    # no data augmentation\n",
    "    else:\n",
    "        data_size = labels.shape[0]\n",
    "        features_all, targets_all = place_holders(data_size, augmentation)\n",
    "        labels.apply(lambda x: extract_features(x,features_all, targets_all), axis = 1)\n",
    "        save_features_targets('feature_all', feature_all)\n",
    "        save_feature_targets ('traget_all', target_all)            \n",
    "    "
=======
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12540, 25088) (12540, 1)\n"
     ]
    }
   ],
   "source": [
    "print (features_train.shape, targets_train.shape)"
>>>>>>> 3335707eac72030c71c4b97b15be68884e3c1270
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "if __name__ == '__main__':\n",
    "    # whether data augmentation is needed or not\n",
    "    agumentation = False\n",
    "    main(agumentation)"
=======
    "# apply data augmentation only on the training part of the data set\n",
    "# for each row of image call extract_features method\n",
    "# this call might take some time, but it is a one time process\n",
    "counter = 0\n",
    "train.apply(lambda x: extract_features(x, features_train,targets_train, True), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = valid.shape[0]\n",
    "features_valid = np.zeros ((data_size*data_agum_size + data_size, 7*7*512))\n",
    "targets_valid = np.zeros ((data_size*data_agum_size + data_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3055, 25088) (3055, 1)\n"
     ]
    }
   ],
   "source": [
    "print (features_valid.shape, targets_valid.shape)"
>>>>>>> 3335707eac72030c71c4b97b15be68884e3c1270
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
   "source": []
=======
   "source": [
    "# do not apply data augmentation on the validation part of the data set\n",
    "counter = 0\n",
    "valid.apply(lambda x: extract_features(x, features_valid,targets_valid), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving features and labels for further training\n",
    "# a classifier will be trained on these features and labels\n",
    "np.save('features_train', features_train)\n",
    "np.save('targets_train', targets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving features and labels for further training\n",
    "# a classifier will be trained these features and their crossponding labels\n",
    "np.save('features_valid', features_valid)\n",
    "np.save('targets_valid', targets_valid)"
   ]
>>>>>>> 3335707eac72030c71c4b97b15be68884e3c1270
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
